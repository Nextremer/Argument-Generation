# -*- coding: utf-8 -*-
# created by Tomohiko Abe
# created at 2018/10/12

import chainer
import chainer.links as L
import chainer.functions as F
from chainer import optimizers, backends

import numpy as np


EOS = 0
UNK = 1
EOL1 = 0
EOL2 = 0


class Scorer(chainer.Chain):
    """encoder hidden statesとdecoder hidden stateのスコア関数"""
    def __init__(self, n_units, attn_n_units):
        super(Scorer, self).__init__()
        with self.init_scope():
            self.W_a = L.Linear(n_units, n_units)
            self.W_e = L.Linear(n_units*2, attn_n_units)
            self.W_d = L.Linear(n_units, attn_n_units)
            self.v = L.Linear(attn_n_units, 1)
            
    def dot(self, eh, dh_t):
        return F.matmul(eh, dh_t[:, None])
    
    def general(self, eh, dh_t):
        return F.matmul(eh, self.W_a(dh_t[None,:]).T)
        
    def concat(self, eh, dh_t):
        T, H = eh.shape
        dh_ = F.expand_dims(dh_t, axis=0)
        dh_ = self.W_d(dh_)
        dh_ = F.repeat(dh_, T, axis=0)
        e = F.tanh(self.W_e(eh)+dh_)
        e = self.v(e)
        return e


class Attention(chainer.Chain):
    
    def __init__(self, n_units, attn_n_units):
        super(Attention, self).__init__()
        with self.init_scope():
            self.W = L.Linear(n_units+n_units*2, n_units)
            self.scorer = Scorer(n_units, attn_n_units)
            
    def __call__(self, ehs, dhs):
        attn_hs = []
        # バッチ内の各sequenceに対してattentional hidden statesを計算
        
        for eh, dh in zip(ehs, dhs):
            hs = []
            # decoderの各hidden stateについてattentionをとる
            for dh_t in dh:
                T, H = eh.shape
                e = self.scorer.concat(eh, dh_t)
                alp = F.softmax(e.T)
                alp = F.repeat(alp.T, H, axis=1)
                h = F.sum(alp * eh, axis=0)[None, :]
                hs.append(h)
            hs = F.concat(hs, axis=0)
            h = F.concat((hs, dh), axis=1)
            h = F.tanh(self.W(h))
            attn_hs.append(h)
        return attn_hs


class Model(chainer.Chain):

    def __init__(self, source_w2id, target_w2id, n_layers, n_units, attn_n_units, eta):
        n_source_vocab = len(source_w2id)
        n_target_vocab = len(target_w2id)
        n_label1 = 3
        n_label2 = 4

        super(Model, self).__init__()
        with self.init_scope():
            self.embed_x = L.EmbedID(n_source_vocab, n_units)
            self.embed_y = L.EmbedID(n_target_vocab, n_units)
            self.encoder = L.NStepBiLSTM(n_layers, n_units, n_units, dropout=0.5)
            self.decoder1 = L.NStepLSTM(n_layers, n_units, n_units, dropout=0.5)
            self.decoder2 = L.NStepLSTM(n_layers, n_units, n_units, dropout=0.5)
            self.W_y = L.Linear(n_units, n_target_vocab)
            self.W_l1 = L.Linear(n_units, n_label1)
            self.W_l2 = L.Linear(n_units, n_label2)
            
            self.attention = Attention(n_units, attn_n_units)
            
        self.eta = eta
        
    def __call__(self, xs, ys, ls):
        lhs, ls1_out, ls2_out, concat_os, concat_ys_out = self.forward(xs, ys, ls)
        lhs = [lh[:-1] for lh in lhs]
        ls1_out = [ls1[:-1] for ls1 in ls1_out]
        ls2_out = [ls2[:-1] for ls2 in ls2_out]
        
        concat_lhs = F.concat(lhs, axis=0)
        concat_ls1_out = F.concat(ls1_out, axis=0)
        concat_ls2_out = F.concat(ls2_out, axis=0)
        
        batchsize = len(xs)
        loss1 = F.sum(F.softmax_cross_entropy(self.W_y(concat_os), concat_ys_out, reduce='no'))/batchsize
        loss2 = F.sum(F.softmax_cross_entropy(self.W_l1(concat_lhs), concat_ls1_out, reduce='no'))/batchsize
        loss3 = F.sum(F.softmax_cross_entropy(self.W_l2(concat_lhs), concat_ls2_out, reduce='no'))/batchsize
        loss = loss1 + self.eta * (loss2 + loss3)
        
        return loss1, loss2, loss3, loss

    def forward(self, xs, ys, ls):
        xs = [self.xp.array(x[::-1], dtype=self.xp.int32) for x in xs]
        ys = [self.xp.array(y, dtype=self.xp.int32) for y in ys]
        
        ls1, ls2 = ls
        ls1 = [self.xp.array(l1, dtype=self.xp.int32) for l1 in ls1]
        ls2 = [self.xp.array(l2, dtype=self.xp.int32) for l2 in ls2]
        
        eos = self.xp.array([EOS], dtype=self.xp.int32)
        eol1 = self.xp.array([EOL1], dtype=self.xp.int32)
        eol2 = self.xp.array([EOL2], dtype=self.xp.int32)
        
        # decoderへの入力
        ys_in = [F.concat([eos, y], axis=0) for y in ys]
        # decoderの目標出力
        ys_out = [F.concat([y, eos], axis=0) for y in ys]
        # label decoderの目標出力
        ls1_out = [F.concat([l1, eol1], axis=0) for l1 in ls1]
        ls2_out = [F.concat([l2, eol2], axis=0) for l2 in ls2]
        assert len(ys_out) == len(ls1_out)
        assert len(ys_out) == len(ls2_out)
        
        # 埋め込みベクトル
        # exs: (batchsize, seq length, n_units)
        exs = self.sequence_embed(self.embed_x, xs)
        eys = self.sequence_embed(self.embed_y, ys_in)
        
        _, _, ehs = self.encoder(None, None, exs)
        _, _, dhs = self.decoder1(None, None, eys)
        
        yhs = self.attention(ehs, dhs)
        
        _, _, lhs = self.decoder2(None, None, yhs)
        
        concat_yhs = F.concat(yhs, axis=0)
        concat_ys_out = F.concat(ys_out, axis=0)
        
        return lhs, ls1_out, ls2_out, concat_yhs, concat_ys_out

    def sequence_embed(self, embed, xs):
        x_len = [len(x) for x in xs]
        x_section = np.cumsum(x_len[:-1])
        ex = embed(F.concat(xs, axis=0))
        exs = F.split_axis(ex, x_section, axis=0)
        return exs
    
    def generate(self, xs, max_length=100):
        batchsize = len(xs)
        with chainer.no_backprop_mode(), chainer.using_config('train', False):
            xs = [self.xp.array(x[::-1]) for x in xs]
            exs = self.sequence_embed(self.embed_x, xs)
            _, _, ehs = self.encoder(None, None, exs)
            ys = self.xp.full(batchsize, EOS, dtype=self.xp.int32)
            h, c = None, None
            result = []
            for i in range(max_length):
                eys = self.embed_y(ys)
                eys = F.split_axis(eys, batchsize, axis=0)
                h, c, dhs = self.decoder1(h, c, eys)
                yhs = self.attention(ehs, dhs)
                concat_yhs = F.concat(yhs, axis=0)
                wy = self.W_y(concat_yhs)
                ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
                result.append(ys)
                
        result = backends.cuda.to_cpu(self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)
        
        # EOSを除去
        outs = []
        for y in result:
            inds = np.argwhere(y == EOS)
            if len(inds) > 0:
                y = y[:inds[0, 0]]
            outs.append(y)
        
        return outs
    
    def get_accuracy(self, xs, ys, ls):
        lhs, ls1_out, ls2_out, _, _ = self.forward(xs, ys, ls)
        lhs = [lh[:-1] for lh in lhs]
        ls1_out = [ls1[:-1] for ls1 in ls1_out]
        ls2_out = [ls2[:-1] for ls2 in ls2_out]
        
        concat_lhs = F.concat(lhs, axis=0)
        concat_ls1_out = F.concat(ls1_out, axis=0)
        concat_ls2_out = F.concat(ls2_out, axis=0)

        ls1_pred = F.argmax(self.W_l1(concat_lhs), axis=1)
        ls2_pred = F.argmax(self.W_l2(concat_lhs), axis=1)
        
        acc = np.sum(np.logical_and(backends.cuda.to_cpu(ls1_pred.data) == backends.cuda.to_cpu(concat_ls1_out.data),\
                                    backends.cuda.to_cpu(ls2_pred.data) == backends.cuda.to_cpu(concat_ls2_out.data)))\
            /float(len(concat_ls1_out))
        return acc
    
    
